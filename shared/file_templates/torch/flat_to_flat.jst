require('torch')
require('nn')

local {{=it.moduleName}}, parent = torch.class('nn.{{=it.moduleName}}', 'nn.Container')

function {{=it.moduleName}}:__init()
    parent.__init(self)

    -- Create a single, feed forward perceptron layer
    self.module = nn.Linear({{=it.inputSize}}, {{=it.outputSize}})

    -- so that it can be handled like a Container
    self:add(self.module)
end


function {{=it.moduleName}}:updateOutput(input)
    self.output = self.module:updateOutput(input)
    return self.output
end

function {{=it.moduleName}}:updateGradInput(input, gradOutput)
    self.gradInput = self.module:updateGradInput(input, gradOutput)
    return self.gradInput
end

function {{=it.moduleName}}:accGradParameters(input, gradOutput, scale)
    self.module:accGradParameters(input, gradOutput, scale)
end

function {{=it.moduleName}}:accUpdateGradParameters(input, gradOutput, lr)
    self.module:accUpdateGradParameters(input, gradOutput, lr)
end

function {{=it.moduleName}}:sharedAccUpdateGradParameters(input, gradOutput, lr)
    self.module:sharedAccUpdateGradParameters(input, gradOutput, lr)
end

function {{=it.moduleName}}:__tostring__()
    if self.module.__tostring__ then
        return torch.type(self) .. ' @ ' .. self.module:__tostring__()
    else
        return torch.type(self) .. ' @ ' .. torch.type(self.module)
    end
end